{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b0c535",
   "metadata": {},
   "source": [
    "# Lecture 6: Loading Models with `from_pretrained`\n",
    "In this notebook, we will explore how to load pre-trained models using the `from_pretrained` method from the Hugging Face Transformers library. We will also dive into the configuration, weights, and caching mechanisms.\n",
    "\n",
    "The Google Colab versin of this notebook is available here: https://colab.research.google.com/drive/1gb3hu83Wktk5cUDObPJqjlM5olAhbvam?usp=sharing\n",
    "\n",
    "\n",
    "## **Feeling Brave?**\n",
    "\n",
    "Try out the code for this lecture on your laptop or desktop. The same code in the above mentioned Google Colab is also available here for anyone who feels they want to take on the challenge of running this lecture on their machine.\n",
    "\n",
    "\n",
    "**Consider the following** before running this notebook on your computer:\n",
    "1. Make sure you have plenty of RAM (ideally >= 16 GB)\n",
    "\n",
    "2. If you **do not have** an NVIDIA GPU, you will have to install bitsandbytes cour version by following these steps\n",
    "    - In your terminal, activate your virtual environment and run `pip uninstall bitsandbytes` or in your notebook cell run `!pip uninstall bitsandbytes`\n",
    "    - In your terminal run `pip install bitsandbytes-cpu` or in your notebook cell run `!pip install bitsandbytes-cpu`\n",
    "\n",
    "3. If you **do** have an NVIDIA GPU\n",
    "    - In your terminal, activate your virtual environment and run `pip uninstall torch torchvision torchaudio` or in your notebook cell run `!pip uninstall torch torchvision torchaudio`\n",
    "    - In your terminal run `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129` or in your notebook cell run `!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129`\n",
    "\n",
    "4. Make sure you have a stable internet connection as the download can take some time\n",
    "\n",
    "*Note* that running models without a GPU can be extremely time consuming and may lead to your machine overheating if it is old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae931b",
   "metadata": {},
   "source": [
    "# Step 1: Load libraries and log in to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2eef16",
   "metadata": {},
   "source": [
    "# Step 2: Load quantization configuration for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370475f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# This is a CPU equivalent of the GPU quantization config\n",
    "# Uncomment the below if you are only using CPU\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=float32,\n",
    "#     bnb_4bit_quant_type=\"nf4\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7509a",
   "metadata": {},
   "source": [
    "# Step 3: Load a Pre-trained Model\n",
    "We will use the `meta-llama/Meta-Llama-3.1-8B-Instruct` model as an example. This step demonstrates how to load the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ee7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quant_config)\n",
    "\n",
    "print(f\"Model '{model_name}' loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea53b6",
   "metadata": {},
   "source": [
    "# Step 4: Explore the Model Configuration\n",
    "The configuration of a model contains important details such as the number of layers, hidden size, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.config\n",
    "print(\"Model Configuration:\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4acc9a",
   "metadata": {},
   "source": [
    "Let's have a look at the actual layers (just for fun!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f042ff",
   "metadata": {},
   "source": [
    "# Step 5: Understand Caching\n",
    "When you load a model, it is cached locally to avoid downloading it again. The models are usually stored in the following path: ~/.cache/huggingface/hub by default\n",
    "\n",
    "See further reference: [Huggingface cache management](~/.cache/huggingface/hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ec82c",
   "metadata": {},
   "source": [
    "# Step 6: Tokenizing a Prompt and Generating Text\n",
    "In this step, we will tokenize a list of messages, pass it to the model, and generate text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e75830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An instruct model requires a list of messages as we saw in AI Engineering Essentials Part 1\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the best way to structure and organize my thoughts?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30732686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cd4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input IDs to the model to generate output\n",
    "output_ids = model.generate(inputs, max_new_tokens=80)\n",
    "\n",
    "# Display the generated output IDs\n",
    "print(\"Generated Output IDs:\", output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4dae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e693f69",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "- The `from_pretrained` method simplifies loading pre-trained models and tokenizers.\n",
    "- Models are cached locally for efficiency.\n",
    "- You can explore model configurations and map models to devices for optimized inference.\n",
    "\n",
    "# Your Challenge\n",
    "Fork this notebook, change the model ID to one in your native language, and share your results in the course repository!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
